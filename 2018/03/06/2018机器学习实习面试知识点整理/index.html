<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习," />





  <link rel="alternate" href="/atom.xml" title="简单点" type="application/atom+xml" />






<meta name="description" content="算是第一次正式的跟职场接触, 刚开始的时候也是一头雾水的状态。面得多了也慢慢有经验, 除了第一个头条挂了, 嗯挂的很惨。也是很想去的一家公司, 但是第一次哎, 多半会这样, 有种似曾相识的感觉。后面的除了阿里还没有回复, 一共拿了5个offer, 还算满意了吧。有些遗憾的是因为时间不合适, 没有去尝试一些大厂, 下次一定要补上。 这里总结的问题一部分是我自己遇到的, 还有一大部分是网上看了别人的">
<meta name="keywords" content="机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="2018机器学习实习面试知识点整理">
<meta property="og:url" content="http://czx.im/2018/03/06/2018机器学习实习面试知识点整理/index.html">
<meta property="og:site_name" content="简单点">
<meta property="og:description" content="算是第一次正式的跟职场接触, 刚开始的时候也是一头雾水的状态。面得多了也慢慢有经验, 除了第一个头条挂了, 嗯挂的很惨。也是很想去的一家公司, 但是第一次哎, 多半会这样, 有种似曾相识的感觉。后面的除了阿里还没有回复, 一共拿了5个offer, 还算满意了吧。有些遗憾的是因为时间不合适, 没有去尝试一些大厂, 下次一定要补上。 这里总结的问题一部分是我自己遇到的, 还有一大部分是网上看了别人的">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/65401032.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/57695817.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/9308923.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/12921290.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/14429617.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/37604864.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/62464763.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/76764892.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/60634589.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/25932847.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/4082036.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/52471608.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/35803593.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/16824201.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/42015632.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/55724328.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/42823180.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/83379060.jpg">
<meta property="og:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/38457430.jpg">
<meta property="og:updated_time" content="2018-03-08T09:40:27.701Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="2018机器学习实习面试知识点整理">
<meta name="twitter:description" content="算是第一次正式的跟职场接触, 刚开始的时候也是一头雾水的状态。面得多了也慢慢有经验, 除了第一个头条挂了, 嗯挂的很惨。也是很想去的一家公司, 但是第一次哎, 多半会这样, 有种似曾相识的感觉。后面的除了阿里还没有回复, 一共拿了5个offer, 还算满意了吧。有些遗憾的是因为时间不合适, 没有去尝试一些大厂, 下次一定要补上。 这里总结的问题一部分是我自己遇到的, 还有一大部分是网上看了别人的">
<meta name="twitter:image" content="http://p59louklr.bkt.clouddn.com/18-3-8/65401032.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"always","offset":12,"b2t":true,"scrollpercent":true,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://czx.im/2018/03/06/2018机器学习实习面试知识点整理/"/>





  <title>2018机器学习实习面试知识点整理 | 简单点</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>
    <a href="https://github.com/czx94"><img style="position: absolute; top: 0; left: 0; border: 0;" src="https://s3.amazonaws.com/github/ribbons/forkme_left_darkblue_121621.png" alt="Fork me on GitHub"></a>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">简单点</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



<script src="https://cdn.bootcss.com/aplayer/1.6.0/APlayer.min.js"></script>
 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://czx.im/2018/03/06/2018机器学习实习面试知识点整理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Zixiang CAI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/czx.jpeg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="简单点">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">2018机器学习实习面试知识点整理</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-06T10:20:07+01:00">
                2018-03-06
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/笔记/" itemprop="url" rel="index">
                    <span itemprop="name">笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
            <!--noindex-->
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/06/2018机器学习实习面试知识点整理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count hc-comment-count" data-xid="2018/03/06/2018机器学习实习面试知识点整理/" itemprop="commentsCount"></span>
                </a>
              </span>
              <!--/noindex-->
            
          

          
          
             <span id="/2018/03/06/2018机器学习实习面试知识点整理/" class="leancloud_visitors" data-flag-title="2018机器学习实习面试知识点整理">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <script src="/assets/js/APlayer.min.js"> </script><p>算是第一次正式的跟职场接触, 刚开始的时候也是一头雾水的状态。面得多了也慢慢有经验, 除了第一个头条挂了, 嗯挂的很惨。也是很想去的一家公司, 但是第一次哎, 多半会这样, 有种似曾相识的感觉。后面的除了阿里还没有回复, 一共拿了5个offer, 还算满意了吧。有些遗憾的是因为时间不合适, 没有去尝试一些大厂, 下次一定要补上。</p>
<h2 id="这里总结的问题一部分是我自己遇到的-还有一大部分是网上看了别人的面经摘抄或者总结出来的。还有一些有意思的问题-因为懒也没有来得及写上-以后会慢慢补充。很多地方可能考虑不周-或者有别的角度-欢迎大家补充或者指出。"><a href="#这里总结的问题一部分是我自己遇到的-还有一大部分是网上看了别人的面经摘抄或者总结出来的。还有一些有意思的问题-因为懒也没有来得及写上-以后会慢慢补充。很多地方可能考虑不周-或者有别的角度-欢迎大家补充或者指出。" class="headerlink" title="这里总结的问题一部分是我自己遇到的, 还有一大部分是网上看了别人的面经摘抄或者总结出来的。还有一些有意思的问题, 因为懒也没有来得及写上, 以后会慢慢补充。很多地方可能考虑不周, 或者有别的角度, 欢迎大家补充或者指出。"></a>这里总结的问题一部分是我自己遇到的, 还有一大部分是网上看了别人的面经摘抄或者总结出来的。还有一些有意思的问题, 因为懒也没有来得及写上, 以后会慢慢补充。很多地方可能考虑不周, 或者有别的角度, 欢迎大家补充或者指出。</h2><h1 id="面试准备"><a href="#面试准备" class="headerlink" title="面试准备:"></a>面试准备:</h1><h2 id="针对简历问题"><a href="#针对简历问题" class="headerlink" title="针对简历问题"></a>针对简历问题</h2><p>项目用到的特征</p>
<ul>
<li>Auto Color Correlogram (acc)<br>颜色自动相关图（color auto-correlogram），它仅仅考察具有相同颜色的像素间的空间关系</li>
<li>Color and Edge Directivity Descriptor (cedd)<br>CEDD结合了图像的颜色和纹理信息，生成一个144位的直方图。<br>1.1.RGB模型转换为HSV模型<br>H （Hue）代表色调，指通过物体传播或从物体射出的颜色，一般在使用中是由颜色名称来标识的。S （Saturation）代表饱和度，表示色调中灰色成分的比例，指颜色的纯度或强度。V （Value）代表亮度.</li>
<li>Color Layout (cl)<br>Color Layout Descriptor是mpeg-7多媒体内容标准描述中一种高效的局部颜色特征描述，在基于内容的图像检索(Content Based Image Retrieval (CBIR) ) 中表现出很好性能，拥有计算成本低，匹配计算速度快，识别准确率高等优点。<br>可以用来做以图搜图</li>
<li><p>Edge Histogram (eh)</p>
</li>
<li><p>Fuzzy Color and Texture Histogram (fcth)<br>模糊颜色和纹理直方图</p>
</li>
<li>Gabor (gabor)<br>Gabor 特征是一种可以用来描述图像纹理信息的特征，Gabor 滤波器的频率和方向与人类的视觉系统类似，特别适合于纹理表示与判别。<br>从图像处理的角度来看，Gabor特征有如下好处：<br>（1）Gabor核函数由于去掉了直流分量，因此对局部光照的变化不敏感，常常被用在要求对光照有适应性的场合；<br>（2）Gabor滤波结果可以反映图像不同尺度、不同方向上的灰度分布信息。一般说来，大尺度滤波可以反映全局性较强的信息，同时可以掩盖图像中噪声的影响；小尺度可以反映比较精细的局部结构，但容易受到噪声影响。</li>
<li><p>Joint descriptor joining CEDD and FCTH in one histogram (jcd)</p>
</li>
<li><p>Scalable Color (sc)<br>基于MPEG-7推荐的可伸缩颜色描述符</p>
</li>
<li>Tamura (tamura)<br>Tamura 纹理特征基于人类对纹理的视觉感知心理学研究，提出6种属性，即：粗糙度、对比度、方向度、线像度、规整度和粗略度。</li>
<li>Local Binary Patterns (lbp)</li>
</ul>
<p>LBP特征(从灰度图提取)<br>下图反应了某个像素的具体的LBP特征值的计算过程，需要注意的是，LBP值是按照顺时针方向组成的二进制数。<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/65401032.jpg" alt=""><br>用公式来定义的话，如下所示：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/57695817.jpg" alt=""><br>其中代表3x3邻域的中心元素，它的像素值为ic，ip代表邻域内其他像素的值。s(x)是符号函数，定义如下：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/9308923.jpg" alt=""></p>
<p>HOG特征(从灰度图提取)<br>方向梯度直方图（Histogram of Oriented Gradient, HOG）<br>计算图像横坐标和纵坐标方向的梯度，并据此计算每个像素位置的梯度方向值；求导操作不仅能够捕获轮廓，人影和一些纹理信息，还能进一步弱化光照的影响。<br>图像中像素点(x,y)的梯度为：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/12921290.jpg" alt=""><br>       最常用的方法是：首先用[-1,0,1]梯度算子对原图像做卷积运算，得到x方向（水平方向，以向右为正方向）的梯度分量gradscalx，然后用[1,0,-1]T梯度算子对原图像做卷积运算，得到y方向（竖直方向，以向上为正方向）的梯度分量gradscaly。然后再用以上公式计算该像素点的梯度大小和方向。<br>（3）为每个细胞单元构建梯度方向直方图<br>（4）把细胞单元组合成大的块（block），块内归一化梯度直方图</p>
<p>ORB特征(提取特征点)<br>ORB：An Efficient Alternative to SIFT or SURF</p>
<h2 id="调参"><a href="#调参" class="headerlink" title="调参"></a>调参</h2><p>目的：偏差和方差的协调！！！bias，variance<br>原则：模型从简单到复杂，避免过拟合<br>传统方法：<br>1）树类分类器<br>n_estimators:子模型数量<br>max_features:每个子树能用到的最大特征。一般去总特征数开方或者log或者全部<br>max_depth:最大深度<br>min_sample_split:<br>min_sample_leaf:<br>神经网络：<br>learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。<br>不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。<br>网络层数： 先从1层开始。<br>每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。<br>batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。<br>clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15<br>dropout： 0.5<br>L2正则：1.0，超过10的很少见。<br>正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。<br>在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。</p>
<h2 id="什么是boosting-tree（加法模型-前向分布）"><a href="#什么是boosting-tree（加法模型-前向分布）" class="headerlink" title="什么是boosting tree（加法模型+前向分布）"></a>什么是boosting tree（加法模型+前向分布）</h2><p><img src="http://p59louklr.bkt.clouddn.com/18-3-8/14429617.jpg" alt=""><br>Boosting方法： Boosting这其实思想相当的简单，大概是，对一份数据，建立M个模型（比如分类），一般这种模型比较简单，称为弱分类器(weak learner)每次分类都将上一次分错的数据权重提高一点再进行分类，这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的成绩。<br>前向分布算法 实际上是一个贪心的算法，也就是在每一步求解弱分类器Φ(m)和其参数w(m)的时候不去修改之前已经求好的分类器和参数。<br>用当前模型的残差，即r=y-fm(x), 来计算下一颗树的参数。</p>
<h2 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h2><p>梯度提升<br>与普通boosting的区别是，利用的是损失函数的负梯度在当前函数的值来拟合回归树。<br>算法：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/37604864.jpg" alt=""></p>
<h2 id="XGB"><a href="#XGB" class="headerlink" title="XGB"></a>XGB</h2><p>传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。<br>xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。<br>xgboost工具支持并行。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。</p>
<h2 id="L1和L2正则为什么可以减弱overfitting"><a href="#L1和L2正则为什么可以减弱overfitting" class="headerlink" title="L1和L2正则为什么可以减弱overfitting"></a>L1和L2正则为什么可以减弱overfitting</h2><p>欠拟合(underfitting)，或者叫作叫做高偏差(bias).过拟合(overfitting)，也叫高方差(variance).<br>越简单的模型泛化能力越强。<br>对高阶项进行一定的惩罚，避免模型在数据量不够的时候过于复杂。<br>Ps：防止过拟合的其他方法，early stopping、数据集扩增（Data augmentation），dropout<br>L1和L2正则有什么区别<br>L1范数和L0范数可以实现稀疏（使没用的特征为0），L1因具有比L0更好的优化求解特性而被广泛应用。L1使损失函数可导。<br>L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。<br>L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。<br>为什么L1正则可以实现参数稀疏，而L2正则不可以？答了：L1正则因为是绝对值形式，很多系数被压缩为0,。而L2正则是很多系数被压迫到接近于0，而不是0<br>为什么L1很多系数可以被压缩为0，L2是被压缩至接近于0？答了：图像上，L1正则是正方形，L2正则是圆形。L1正则的往往取到正方形顶点，即有很多参数为0.L2正则往往去不到圆形和参数线的交点，即很多分量被压缩到接近于0</p>
<h2 id="怎么理解dropout"><a href="#怎么理解dropout" class="headerlink" title="怎么理解dropout"></a>怎么理解dropout</h2><p>在每次训练的时候使用dropout，每个神经元有一定的概率被移除，这样可以使得一个神经元的训练不依赖于另外一个神经元，同样也就使得特征之间的协同作用被减弱。Hinton认为，过拟合可以通过阻止某些特征的协同作用来缓解。增加鲁棒性。<br>也可以理解为相当于在训练不同的网络，最后投票来决定结果。</p>
<h2 id="KNN和Lr有什么本质区别"><a href="#KNN和Lr有什么本质区别" class="headerlink" title="KNN和Lr有什么本质区别"></a>KNN和Lr有什么本质区别</h2><p>LR属于线性模型。<br>因为 logistic 回归的决策边界（decision boundary）是线性的。<br>KNN属于非线性模型</p>
<h2 id="学习率如何影响训练？"><a href="#学习率如何影响训练？" class="headerlink" title="学习率如何影响训练？"></a>学习率如何影响训练？</h2><p>如果学习率很低，训练会变得更加可靠，但是优化会耗费较长的时间，因为朝向损失函数最小值的每个步长很小。<br>如果学习率很高，训练可能根本不会收敛，甚至会发散。权重的改变量可能非常大，使得优化越过最小值，使得损失函数变得更糟。</p>
<h2 id="如何解决样本不均衡的问题"><a href="#如何解决样本不均衡的问题" class="headerlink" title="如何解决样本不均衡的问题?"></a>如何解决样本不均衡的问题?</h2><p>从数据集外补充, 我的项目里直接从网上爬图片, 加上SMOTE等过采样的算法(有好多种, 各有优劣, SMOTE应该是最好的), 以及对图片进行一些几何上的处理。</p>
<h2 id="线性分类器和非线性分类器"><a href="#线性分类器和非线性分类器" class="headerlink" title="线性分类器和非线性分类器"></a>线性分类器和非线性分类器</h2><p>线性分类器：模型是参数的线性函数，分类平面是（超）平面；<br>非线性分类器：模型分界面可以是曲面或者超平面的组合。<br>典型的线性分类器有感知机，LDA，逻辑斯特回归，SVM（线性核；<br>典型的非线性分类器有朴素贝叶斯（有文章说这个本质是线性的，<a href="http://dataunion.org/12344.html），kNN，决策树，SVM（非线性核）" target="_blank" rel="noopener">http://dataunion.org/12344.html），kNN，决策树，SVM（非线性核）</a></p>
<h2 id="为什么random-forest具有特征选择的功能"><a href="#为什么random-forest具有特征选择的功能" class="headerlink" title="为什么random forest具有特征选择的功能"></a>为什么random forest具有特征选择的功能</h2><p>树类分类器其实都可以, 因为可以判断每个特征对应的信息增益, 信息增益越大的特征分类效果理论上来说就越好。在非结构话数据的项目中常常会用这种方法来选取有用的特征, 从而降低特征维度。在特特征维度高的时候还是特别好用的。</p>
<h2 id="random-forest有哪些重要的参数？"><a href="#random-forest有哪些重要的参数？" class="headerlink" title="random forest有哪些重要的参数？"></a>random forest有哪些重要的参数？</h2><p>A. max_features：<br>随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：<br>Auto/None ：简单地选取所有特征，每颗树都可以利用他们。这种情况下，每颗树都没有任何的限制。<br>sqrt ：此选项是每颗子树可以利用总特征数的平方根个。 例如，如果变量（特征）的总数是100，所以每颗子树只能取其中的10个。“log2”是另一种相似类型的选项。<br>0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20％。如果想考察的特征x％的作用， 我们可以使用“0.X”的格式。<br>max_features如何影响性能和速度？<br>增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。<br>B. n_estimators：<br>在利用最大投票数或平均值来预测之前，你想要建立子树的数量。 较多的子树可以让模型有更好的性能，但同时让你的代码变慢。 你应该选择尽可能高的值，只要你的处理器能够承受的住，因为这使你的预测更好更稳定。<br>C. min_sample_leaf：<br>如果您以前编写过一个决策树，你能体会到最小样本叶片大小的重要性。 叶是决策树的末端节点。 较小的叶子使模型更容易捕捉训练数据中的噪声。 一般来说，我更偏向于将最小叶子节点数目设置为大于50。在你自己的情况中，你应该尽量尝试多种叶子大小种类，以找到最优的那个。</p>
<h2 id="为什么梯度反方向是函数值局部下降最快的方向？"><a href="#为什么梯度反方向是函数值局部下降最快的方向？" class="headerlink" title="为什么梯度反方向是函数值局部下降最快的方向？"></a>为什么梯度反方向是函数值局部下降最快的方向？</h2><p><img src="http://p59louklr.bkt.clouddn.com/18-3-8/62464763.jpg" alt=""><br>那么此时如果要取得最大值，也就是当为0度的时候，也就是向量(这个方向是一直在变，在寻找一个函数变化最快的方向）与向量（这个方向当点固定下来的时候，它就是固定的）平行的时候，方向导数最大.方向导数最大，也就是单位步伐，函数值朝这个反向变化最快.</p>
<h2 id="DNN为什么功能强大，说说你的理解"><a href="#DNN为什么功能强大，说说你的理解" class="headerlink" title="DNN为什么功能强大，说说你的理解"></a>DNN为什么功能强大，说说你的理解</h2><p>深度学习的深度一方面增加了大量的参数，增加的参数意味着这个网络的表达能力更强大了。可以学习和区分的特征更多了。而一旦学习到的特征变多的话，我们在分类和识别的能力也就变好了。<br>从简单特征到抽象特征。随着网络深度增加，提取的特征不断复杂化。更能理解复杂概念。</p>
<h2 id="SVM的损失函数是什么？怎么理解"><a href="#SVM的损失函数是什么？怎么理解" class="headerlink" title="SVM的损失函数是什么？怎么理解"></a>SVM的损失函数是什么？怎么理解</h2><p>2分类SVM等于Hinge损失 + L2正则化。SVM最大化分类间距的目标函数等价于最小化Hinge损失 + L2正则化。推导并不复杂, 详见《统计学习方法》。</p>
<h2 id="介绍下Maxout"><a href="#介绍下Maxout" class="headerlink" title="介绍下Maxout"></a>介绍下Maxout</h2><p>maxout激活函数，它具有如下性质：<br>1、maxout激活函数并不是一个固定的函数，不像Sigmod、Relu、Tanh等函数，是一个固定的函数方程<br>2、它是一个可学习的激活函数，因为我们W参数是学习变化的。<br>3、它是一个分段线性函数：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/76764892.jpg" alt=""><br>maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合（学过高等数学应该能明白），而maxout又是取k个隐隐含层节点的最大值，这些”隐隐含层”节点也是线性的，所以在不同的取值范围下，最大值也可以看做是分段线性的（分段的个数与k值有关）</p>
<h2 id="根据混淆矩阵可以得到评价分类模型的指标有以下几种。"><a href="#根据混淆矩阵可以得到评价分类模型的指标有以下几种。" class="headerlink" title="根据混淆矩阵可以得到评价分类模型的指标有以下几种。"></a>根据混淆矩阵可以得到评价分类模型的指标有以下几种。</h2><p>分类准确度，就是正负样本分别被正确分类的概率，计算公式为：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/60634589.jpg" alt=""><br>召回率，就是正样本被识别出的概率，计算公式为：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/25932847.jpg" alt=""><br>虚警率，就是负样本被错误分为正样本的概率，计算公式为：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/4082036.jpg" alt=""><br>精确度，就是分类结果为正样本的情况真实性程度，计算公式为：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/52471608.jpg" alt=""></p>
<h2 id="优化器类型"><a href="#优化器类型" class="headerlink" title="优化器类型"></a>优化器类型</h2><ol>
<li>Batch gradient descent<br>梯度更新规则:<br>BGD 采用整个训练集的数据来计算 cost function 对参数的梯度：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/35803593.jpg" alt=""><br>Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。</li>
<li>Stochastic gradient descent<br>梯度更新规则:<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/16824201.jpg" alt=""><br>和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，<br>对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，<br>而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。<br>缺点:<br>但是 SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。<br>BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。</li>
</ol>
<p>3.Adam<br>自适应优化器，能够自发地改变学习率。效果最好。<br>存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的指数衰减平均值：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/42015632.jpg" alt=""><br>梯度更新规则:<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/55724328.jpg" alt=""><br>超参数设定值:<br>建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8<br>实践表明，Adam 比其他适应性学习方法效果要好</p>
<h2 id="什么是梯度消失？怎么解决"><a href="#什么是梯度消失？怎么解决" class="headerlink" title="什么是梯度消失？怎么解决"></a>什么是梯度消失？怎么解决</h2><p>梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/42823180.jpg" alt=""><br>其实梯度爆炸和梯度消失问题都是因为网络太深，链式求导，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。</p>
<h2 id="batch-normalisation作用"><a href="#batch-normalisation作用" class="headerlink" title="batch normalisation作用"></a>batch normalisation作用</h2><p>那BN到底是什么原理呢？说到底还是为了防止“梯度弥散”。关于梯度弥散，大家都知道一个简单的栗子：<br><img src="http://p59louklr.bkt.clouddn.com/18-3-8/83379060.jpg" alt=""><br>在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法。</p>
<h2 id="LSTM模型介绍和BPTT推导"><a href="#LSTM模型介绍和BPTT推导" class="headerlink" title="LSTM模型介绍和BPTT推导"></a>LSTM模型介绍和BPTT推导</h2><p><img src="http://p59louklr.bkt.clouddn.com/18-3-8/38457430.jpg" alt=""></p>
<h2 id="LR-和-linear-SVM区别"><a href="#LR-和-linear-SVM区别" class="headerlink" title="LR 和 linear SVM区别"></a>LR 和 linear SVM区别</h2><p>相同点：<br>1，LR和SVM都是分类算法。<br>2，如果不考虑核函数，LR和SVM都是线性分类算法，即分类决策面都是线性的。<br>3，LR和SVM都是监督学习算法。<br>不同点：<br>1，本质上是其loss function不同。<br>2，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。<br>线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。<br>3，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。<br>这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。<br>4，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。<br>5，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！<br>在Andrew NG的课里讲到过：</p>
<ol>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ol>
<h2 id="SVM面试问题"><a href="#SVM面试问题" class="headerlink" title="SVM面试问题"></a>SVM面试问题</h2><p>为什么要把原问题转换为对偶问题？因为原问题是凸二次规划问题，转换为对偶问题更加高效。<br>为什么求解对偶问题更加高效？因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0.<br>alpha系数有多少个？样本点的个数</p>
<h2 id="线性回归基本假设"><a href="#线性回归基本假设" class="headerlink" title="线性回归基本假设?"></a>线性回归基本假设?</h2><p>线性回归需要满足四个前提假设.LINE!!!</p>
<ol>
<li>Linearity 线性. 应变量和每个自变量都是线性关系。 </li>
<li>Indpendence 独立性. 对于所有的观测值，它们的误差项相互之间是独立的。 </li>
<li>Normality 正态性. 误差项服从正态分布。 </li>
<li>Equal-variance 等方差. 所有的误差项具有同样方差。</li>
</ol>
<h2 id="用cos做激活函数行不行"><a href="#用cos做激活函数行不行" class="headerlink" title="用cos做激活函数行不行?"></a>用cos做激活函数行不行?</h2><p>答案是可以的, 面试的时候想太多答得不好。当时觉得cos容易使得模型出现梯度消失的情况, 但是其实要看具体的问题, cos在某些问题上表现得很好。其实激活函数的目的是为了使模型具有非线性, 否则再深的神经网络到最后也只是一个线性分类器。<br>一个好的激活函数应该有如下几个标准(摘自知乎):<br>作者：Hengkai Guo<br>链接：<a href="https://www.zhihu.com/question/67366051/answer/262087707" target="_blank" rel="noopener">https://www.zhihu.com/question/67366051/answer/262087707</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<ol>
<li>非线性：即导数不是常数。这个条件前面很多答主都提到了，是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。</li>
<li>几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。</li>
<li>计算简单：正如题主所说，非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。</li>
<li>非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。</li>
<li>单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。</li>
<li>输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。7. 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。</li>
<li>参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。</li>
<li>归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。</li>
</ol>
<h2 id="To-be-solved…"><a href="#To-be-solved…" class="headerlink" title="To be solved…"></a>To be solved…</h2><p>项目中over-fitting了，你怎么办<br>详细说一个你知道的优化算法(Adam等)<br>项目(比赛）怎么做的模型的ensemble<br>stacking是什么？需要注意哪些问题<br>了解哪些online learning的算法<br>如何解决样本不均衡的问题<br>fasterRCNN中的ROIPooling是如何实现的<br>如何进行特征的选择<br>如何进行模型的选择<br>常用的有哪些损失函数<br>XX用户画像挖掘怎么做的feature engineering?<br>假设一个5*5的filter与图像卷积，如何降低计算量？<br>做过模型压缩吗？介绍下<br>什么是residual learning？说说你的理解<br>residual learning所说的residual和GBDT中的residual有什么区别？<br>FFM和FTRL有过了解吗？<br>你对现在Deep Learning的发展和遇到的问题有什么看法？</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>Why so serious? Just for fun！</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.jpeg" alt="Zixiang CAI 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.jpeg" alt="Zixiang CAI 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/05/写在开始/" rel="next" title="写在开始">
                <i class="fa fa-chevron-left"></i> 写在开始
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/03/07/Tensorflow学习笔记0（简介）/" rel="prev" title="Tensorflow学习笔记0（简介）">
                Tensorflow学习笔记0（简介） <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
<span class="jiathis_txt">分享到：</span>
<a class="jiathis_button_fav">收藏夹</a>
<a class="jiathis_button_copy">复制网址</a>
<a class="jiathis_button_email">邮件</a>
<a class="jiathis_button_weixin">微信</a>
<a class="jiathis_button_qzone">QQ空间</a>
<a class="jiathis_button_tqq">腾讯微博</a>
<a class="jiathis_button_douban">豆瓣</a>
<a class="jiathis_button_share">一键分享</a>

<a href="http://www.jiathis.com/share?uid=2140465" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank">更多</a>
<a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript" >
var jiathis_config={
  data_track_clickback:true,
  summary:"",
  shortUrl:false,
  hideMore:false
}
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js?uid=" charset="utf-8"></script>
<!-- JiaThis Button END -->
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="hypercomments_widget"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/czx.jpeg"
                alt="Zixiang CAI" />
            
              <p class="site-author-name" itemprop="name">Zixiang CAI</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">2</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">3</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#这里总结的问题一部分是我自己遇到的-还有一大部分是网上看了别人的面经摘抄或者总结出来的。还有一些有意思的问题-因为懒也没有来得及写上-以后会慢慢补充。很多地方可能考虑不周-或者有别的角度-欢迎大家补充或者指出。"><span class="nav-number">1.</span> <span class="nav-text">这里总结的问题一部分是我自己遇到的, 还有一大部分是网上看了别人的面经摘抄或者总结出来的。还有一些有意思的问题, 因为懒也没有来得及写上, 以后会慢慢补充。很多地方可能考虑不周, 或者有别的角度, 欢迎大家补充或者指出。</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#面试准备"><span class="nav-number"></span> <span class="nav-text">面试准备:</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#针对简历问题"><span class="nav-number">1.</span> <span class="nav-text">针对简历问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#调参"><span class="nav-number">2.</span> <span class="nav-text">调参</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是boosting-tree（加法模型-前向分布）"><span class="nav-number">3.</span> <span class="nav-text">什么是boosting tree（加法模型+前向分布）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#GBDT"><span class="nav-number">4.</span> <span class="nav-text">GBDT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#XGB"><span class="nav-number">5.</span> <span class="nav-text">XGB</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#L1和L2正则为什么可以减弱overfitting"><span class="nav-number">6.</span> <span class="nav-text">L1和L2正则为什么可以减弱overfitting</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#怎么理解dropout"><span class="nav-number">7.</span> <span class="nav-text">怎么理解dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#KNN和Lr有什么本质区别"><span class="nav-number">8.</span> <span class="nav-text">KNN和Lr有什么本质区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#学习率如何影响训练？"><span class="nav-number">9.</span> <span class="nav-text">学习率如何影响训练？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#如何解决样本不均衡的问题"><span class="nav-number">10.</span> <span class="nav-text">如何解决样本不均衡的问题?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性分类器和非线性分类器"><span class="nav-number">11.</span> <span class="nav-text">线性分类器和非线性分类器</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么random-forest具有特征选择的功能"><span class="nav-number">12.</span> <span class="nav-text">为什么random forest具有特征选择的功能</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#random-forest有哪些重要的参数？"><span class="nav-number">13.</span> <span class="nav-text">random forest有哪些重要的参数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#为什么梯度反方向是函数值局部下降最快的方向？"><span class="nav-number">14.</span> <span class="nav-text">为什么梯度反方向是函数值局部下降最快的方向？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#DNN为什么功能强大，说说你的理解"><span class="nav-number">15.</span> <span class="nav-text">DNN为什么功能强大，说说你的理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVM的损失函数是什么？怎么理解"><span class="nav-number">16.</span> <span class="nav-text">SVM的损失函数是什么？怎么理解</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍下Maxout"><span class="nav-number">17.</span> <span class="nav-text">介绍下Maxout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#根据混淆矩阵可以得到评价分类模型的指标有以下几种。"><span class="nav-number">18.</span> <span class="nav-text">根据混淆矩阵可以得到评价分类模型的指标有以下几种。</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#优化器类型"><span class="nav-number">19.</span> <span class="nav-text">优化器类型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是梯度消失？怎么解决"><span class="nav-number">20.</span> <span class="nav-text">什么是梯度消失？怎么解决</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#batch-normalisation作用"><span class="nav-number">21.</span> <span class="nav-text">batch normalisation作用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LSTM模型介绍和BPTT推导"><span class="nav-number">22.</span> <span class="nav-text">LSTM模型介绍和BPTT推导</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LR-和-linear-SVM区别"><span class="nav-number">23.</span> <span class="nav-text">LR 和 linear SVM区别</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SVM面试问题"><span class="nav-number">24.</span> <span class="nav-text">SVM面试问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归基本假设"><span class="nav-number">25.</span> <span class="nav-text">线性回归基本假设?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#用cos做激活函数行不行"><span class="nav-number">26.</span> <span class="nav-text">用cos做激活函数行不行?</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#To-be-solved…"><span class="nav-number">27.</span> <span class="nav-text">To be solved…</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      
<div id="music163player">
    <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=280 height=86 src="//music.163.com/outchain/player?type=2&id=38358214&auto=0&height=66">
    </iframe>
</div>
    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zixiang CAI</span>

  
</div>
<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数:<span id="busuanzi_value_site_uv"></span>
</span>
</div>







<script src="https://cdn.jsdelivr.net/npm/meting@1.0.1/dist/Meting.min.js"></script>

        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  







  
  





  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/three.min.js"></script>
  

  
  
    <script type="text/javascript" src="/lib/three/canvas_sphere.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	

		<script type="text/javascript">
		_hcwp = window._hcwp || [];

		_hcwp.push({widget:"Bloggerstream", widget_id: 102317, selector:".hc-comment-count", label: "{\%COUNT%\}" });

		
		_hcwp.push({widget:"Stream", widget_id: 102317, xid: "2018/03/06/2018机器学习实习面试知识点整理/"});
		

		(function() {
		if("HC_LOAD_INIT" in window)return;
		HC_LOAD_INIT = true;
		var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
		var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
		hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/102317/"+lang+"/widget.js";
		var s = document.getElementsByTagName("script")[0];
		s.parentNode.insertBefore(hcc, s.nextSibling);
		})();
		</script>

	


















  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("G3FA02EUDFVx4GyFR2vkG013-gzGzoHsz", "ujXErOLaXjFfHATAopGUwEXY");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  

  

  

</body>
</html>
