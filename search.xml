<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tensorflow学习笔记5(Alexnet实现）]]></title>
    <url>%2F2018%2F03%2F10%2FTensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B05(Alexnet%E5%AE%9E%E7%8E%B0%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Alexnet说好了不说神经网络的内容，但是Alexnet确实太经典太重要了，可以说这一波AI的崛起就是从这个模型开始的，还是得说两句。ImageNet Classification with Deep Convolutional Neural Networks是Hinton和他的学生Alex Krizhevsky在12年ImageNet Challenge使用的模型结构，刷新了Image Classification的几率，从此deep learning在Image这块开始一次次超过state-of-art，甚至于搭到打败人类的地步。先看看Alexnet的创新点： Data Augmentation最简单有效的提高结果，防止过拟合的方法。常用的方法有，几何特征方面：水平翻转、随机剪裁、平移变换；颜色纹理方面：颜色光照变换，如增加减少亮度，一些滤光算法等等之类的。 Dropout LRN ReLU 再看看结构图：alexnet总共包括8层，其中前5层convolutional，后面3层是full-connected，文章里面说的是减少任何一个卷积结果会变得很差，下面我来具体讲讲每一层的构成： 第一层卷积层 输入图像为227*227*3(paper上貌似有点问题224*224*3)的图像，使用了96个kernels（96,11,11,3），以4个pixel为一个单位来右移或者下移，能够产生5555个卷积后的矩形框值，然后进行response-normalized（其实是Local Response Normalized，后面我会讲下这里）和pooled之后，pool这一层好像caffe里面的alexnet和paper里面不太一样，alexnet里面采样了两个GPU，所以从图上面看第一层卷积层厚度有两部分，池化pool_size=(3,3),滑动步长为2个pixels，得到96个2727个feature。 第二层卷积层使用256个（同样，分布在两个GPU上，每个128kernels（5*5*48）），做pad_size(2,2)的处理，以1个pixel为单位移动（感谢网友指出），能够产生27*27个卷积后的矩阵框，做LRN处理，然后pooled，池化以3*3矩形框，2个pixel为步长，得到256个13*13个features。 第三层、第四层都没有LRN和pool，第五层只有pool，其中第三层使用384个kernels（3*3*384，pad_size=(1,1),得到384*15*15，kernel_size为（3，3),以1个pixel为步长，得到384*13*13）；第四层使用384个kernels（pad_size(1,1)得到384*15*15，核大小为（3，3）步长为1个pixel，得到384*13*13）；第五层使用256个kernels（pad_size(1,1)得到384*15*15，kernel_size(3,3)，得到256*13*13，pool_size(3，3）步长2个pixels，得到256*6*6）。 全连接层： 前两层分别有4096个神经元，最后输出softmax为1000个（ImageNet），注意caffe图中全连接层中有relu、dropout、innerProduct。 引用https://zhuanlan.zhihu.com/p/30486789http://www.cnblogs.com/52machinelearning/p/5821591.html]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记4(Tensorboard与可视化）]]></title>
    <url>%2F2018%2F03%2F09%2FTensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B04(Tensorboard%E4%B8%8E%E5%8F%AF%E8%A7%86%E5%8C%96%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Tensorboard的可视化过程（1）首先肯定是先建立一个graph,你想从这个graph中获取某些数据的信息（2）确定要在graph中的哪些节点放置summary operations以记录信息使用tf.summary.scalar记录标量使用tf.summary.histogram记录数据的直方图使用tf.summary.distribution记录数据的分布图使用tf.summary.image记录图像数据（3）operations并不会去真的执行计算，除非你告诉他们需要去run,或者它被其他的需要run的operation所依赖。而我们上一步创建的这些summary operations其实并不被其他节点依赖，因此，我们需要特地去运行所有的summary节点。但是呢，一份程序下来可能有超多这样的summary 节点，要手动一个一个去启动自然是及其繁琐的，因此我们可以使用tf.summary.merge_all去将所有summary节点合并成一个节点，只要运行这个节点，就能产生所有我们之前设置的summary data。（4）使用tf.summary.FileWriter将运行后输出的数据都保存到本地磁盘中（5）运行整个程序，并在终端输入运行tensorboard的指令（tensorboard –logdir logs所在文件夹）。执行成功应该能看到TensorBoard 1.6.0 at http://你的电脑名字:6006 (Press CTRL+C to quit)。之后打开web端可查看可视化的结果。 举个栗子用的还是上一次分类mnist的cnn，但是注意在变量的声明阶段要加上name_scope。声明变量时加的tf.summary.histogram，可以让我们在最后看到训练过程中变量统计直方图的变化。加了一层卷积的目的是为了把特征减少到4个，用tf.summary.image看看卷积网络提取的特征长什么样子（结果也看不出来什么）。因为tf.summary.image只能存储1,3,4维特征的tensor，就是说tensor的第四个数必须是1,3,4之一。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tfdef weight_variable(shape, name): initial = tf.truncated_normal(shape, stddev=0.1, name=name) return tf.Variable(initial)def bias_variable(shape, name): initial = tf.constant(0.1, shape=shape, name=name) return tf.Variable(initial)# 读入mnist数据集mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)# 变量声明# define placeholder for inputs to networkwith tf.name_scope('inputs'): x = tf.placeholder('float', [None, 784], name='x_input') # None 为了留给后面的batch_size y_ = tf.placeholder('float', [None, 10], name='y_input')with tf.name_scope('conv1'): with tf.name_scope('weights'): W_conv1 = weight_variable([5, 5, 1, 32], name='W1') tf.summary.histogram('conv1/weights', W_conv1) with tf.name_scope('bias'): b_conv1 = bias_variable([32], name='B1') tf.summary.histogram('conv1/bias', b_conv1)with tf.name_scope('conv2'): with tf.name_scope('weights'): W_conv2 = weight_variable([5, 5, 32, 64], name='W2') tf.summary.histogram('conv2/weights', W_conv2) with tf.name_scope('bias'): b_conv2 = bias_variable([64], name='B2') tf.summary.histogram('conv2/bias', b_conv2)with tf.name_scope('conv3'): with tf.name_scope('weights'): W_conv3 = weight_variable([5, 5, 64, 4], name='W3') tf.summary.histogram('conv2/weights', W_conv3) with tf.name_scope('bias'): b_conv3 = bias_variable([4], name='B3') tf.summary.histogram('conv3/bias', b_conv3)with tf.name_scope('fc1'): with tf.name_scope('weights'): W_fc1 = weight_variable([7 * 7 * 4, 1024], name='w1') tf.summary.histogram('fc1/weights', W_fc1) with tf.name_scope('bias'): b_fc1 = bias_variable([1024], name='b1') tf.summary.histogram('fc1/weights', b_fc1)with tf.name_scope('fc2'): with tf.name_scope('weights'): W_fc2 = weight_variable([1024, 10], name='w2') tf.summary.histogram('fc2/weights', W_fc2) with tf.name_scope('bias'): b_fc2 = bias_variable([10], name='b2') tf.summary.histogram('fc2/weights', b_fc2) 接下来是计算图的部分。这里存的变量是两个image和两个scalar，其他的name_scope是为了之后在tensorboard可视化计算图用的。123456789101112131415161718192021222324252627282930313233343536# 计算图with tf.name_scope('input_reshape'): x_image = tf.reshape(x, [-1, 28, 28, 1]) tf.summary.image('input', x_image, 10)with tf.name_scope('c1'): conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1,1,1,1], padding='SAME') + b_conv1) pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')with tf.name_scope('c2'): conv2 = tf.nn.relu(tf.nn.conv2d(pool1, W_conv2, strides=[1,1,1,1], padding='SAME') + b_conv2) pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')with tf.name_scope('c3'): conv3 = tf.nn.relu(tf.nn.conv2d(pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3) tf.summary.image('third_conv', conv3, 10)with tf.name_scope('f1'): pool2_flat = tf.reshape(conv3, [-1, 7*7*4]) fc1 = tf.nn.relu(tf.matmul(pool2_flat,W_fc1) + b_fc1)with tf.name_scope('f2'): y = tf.nn.softmax(tf.matmul(fc1, W_fc2) + b_fc2)with tf.name_scope('loss'): loss = -tf.reduce_sum(y_ * tf.log(y)) tf.summary.scalar('loss', loss)with tf.name_scope('train'): train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))with tf.name_scope('accuracy'): accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float')) tf.summary.scalar('accuracy', accuracy) 在训练过程中同时记录模型在训练集和测试集上的表现，所以用了两个tf.summary.FileWriter。其他的训练步骤和之前一样。1234567891011121314151617181920212223242526# 初始化变量init = tf.global_variables_initializer()#logsmerged_summary_op = tf.summary.merge_all()# 开始训练with tf.Session() as sess: sess.run(init) train_summary_writer = tf.summary.FileWriter('./logs', sess.graph) test_summary_writer = tf.summary.FileWriter('./logs', sess.graph) for i in range(1000): x_batch, y_batch = mnist.train.next_batch(50) sess.run(train_step, feed_dict=&#123;x: x_batch, y_: y_batch&#125;) if i % 100 == 0: print(sess.run(accuracy, feed_dict=&#123;x: x_batch, y_: y_batch&#125;)) #eval on train set summary_str = sess.run(merged_summary_op, feed_dict=&#123;x: x_batch, y_: y_batch&#125;) train_summary_writer.add_summary(summary_str, i) #eval on test set summary_str = sess.run(merged_summary_op, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;) test_summary_writer.add_summary(summary_str, i) print ("test accuracy %g"%accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels&#125;)) Tensorboard Web端 SCALAR展示的是标量的信息，我程序中用tf.summary.scalars()定义的信息都会在这个窗口。回顾本文程序中定义的标量有：准确率accuracy,dropout的保留率，隐藏层中的参数信息，已经交叉熵损失。这些都在SCLARS窗口下显示出来了。展示的是标量的信息，我程序中用tf.summary.scalars()定义的信息都会在这个窗口。程序中定义的标量有：accuracy, loss。这些都在SCALARS窗口下显示出来了。老实说要看这两个用tensorboard有点大材小用，有非常方便的解决方案。 IMAGES在程序中我们设置了一处保存了图像信息，就是在转变了输入特征的shape，然后记录到了image中，于是在tensorflow中就会还原出原始的图片了：原始图片28×28×1：经过三层卷积后774, 这个已经看不出特征了，也许该想个更好的方法。或者这时候特征已经比较抽象： GRAPHS这里展示的是整个训练过程的计算图graph,从中我们可以清洗地看到整个程序的逻辑与过程。对于比较深或者结构比较复杂的神经网络，这个工具用来做可视化还是很方便的。单击某个节点，可以查看属性，输入，输出等信息 DISTRIBUTIONS这里查看的是神经元输出的分布，有激活函数之前的分布，激活函数之后的分布等。 HISTOGRAMS也可以看以上保存的变量的直方图。 引用http://blog.csdn.net/u012052268/article/details/75394077http://blog.csdn.net/sinat_33761963/article/details/62433234]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记3(mnist和cnn)]]></title>
    <url>%2F2018%2F03%2F08%2FTensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B03(mnist%E5%92%8Ccnn)%2F</url>
    <content type="text"><![CDATA[MNIST数据集简而言之就是手写数字识别，在所有教程里面都会把这个拿来入门用。这里记录一下自己用fc和CNN在tensorflow上的代码。结果就不重要了，主要为了练手。 读入数据直接从tf模块里读入mnist1234from tensorflow.examples.tutorials.mnist import input_dataimport tensorflow as tf#读入mnist数据集mnist = input_data.read_data_sets("MNIST_data/", one_hot=True) 一层全连接网络12345678910111213141516171819202122232425#变量声明x = tf.placeholder('float',[None, 784]) #None 为了留给后面的batch_sizey = tf.placeholder('float',[None, 10])W = tf.Variable(tf.zeros([784,10]))b = tf.Variable(tf.zeros([10]))#计算图y = tf.nn.softmax(tf.matmul(x, W)+b)loss = -tf.reduce_sum(y_*tf.log(y))train_step = tf.train.AdamOptimizer(0.01).minimize(loss)correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))accuracy = tf.reduce_mean(correct_prediction, 'float')#初始化变量init = tf.global_variables_initializer()# 开始训练with tf.Session() as sess: sess.run(init) for i in range(1000): x_batch, y_batch = mnist.train.next_batch(50) sess.run(train_step, feed_dict=&#123;x: x_batch, y_: y_batch&#125;) if i % 100 == 0: print(sess.run(accuracy, feed_dict=&#123;x: x_batch, y_: y_batch&#125;))#测试 print ("test accuracy %g"%accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels&#125;)) 卷积网络原理和结构很简单，但是自己写还是容易错，写一遍就当复习了。也是有段时间没有写tf底层的结构了。初始化变量，这里用了两层卷积加两层fc，最后softmax分类。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# 变量声明#输入输出占位变量x = tf.placeholder('float', [None, 784]) # None 为了留给后面的batch_sizey_ = tf.placeholder('float', [None, 10])#这里是两层卷积，都用5*5卷积核w_conv1 = tf.Variable(tf.truncated_normal([5,5,1,32], stddev=0.1))b1 = tf.Variable(tf.constant(0.1, shape = [32]))w_conv2 = tf.Variable(tf.truncated_normal([5,5,32,64], stddev=0.1))b2 = tf.Variable(tf.constant(0.1, shape = [64]))#接两层全连接w_fc1 = tf.Variable(tf.truncated_normal([7*7*64, 1024], stddev=0.1))b_fc1 = tf.Variable(tf.constant(0.1, shape = [1024]))W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1))b_fc2 = tf.Variable(tf.constant(0.1, shape = [10]))#引入dropoutkeep_prob = tf.placeholder("float")#计算图#先把输入reshape成一个二维变量x_image = tf.reshape(x, [-1, 28, 28, 1])#第一层卷积，用relu作activationconv1 = tf.nn.relu(tf.nn.conv2d(x_image, w_conv1, strides=[1,1,1,1], padding='SAME'))pool1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')#第二层卷积conv2 = tf.nn.relu(tf.nn.conv2d(pool1, w_conv2, strides=[1,1,1,1], padding='SAME'))pool2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')#flatten后接到全连接层pool2_flat = tf.reshape(pool2, [-1, 7*7*64])fc1 = tf.nn.relu(tf.matmul(pool2_flat,w_fc1) + b_fc1)#最后用softmax分类y = tf.nn.softmax(tf.matmul(fc1, W_fc2) + b_fc2)#同之前的网络loss = -tf.reduce_sum(y_ * tf.log(y))train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))# 初始化变量init = tf.global_variables_initializer()# 开始训练with tf.Session() as sess: sess.run(init) for i in range(10000): x_batch, y_batch = mnist.train.next_batch(50) sess.run(train_step, feed_dict=&#123;x: x_batch, y_: y_batch&#125;) if i % 100 == 0: print(sess.run(accuracy, feed_dict=&#123;x: x_batch, y_: y_batch&#125;)) print ("test accuracy %g"%accuracy.eval(feed_dict=&#123; x: mnist.test.images, y_: mnist.test.labels&#125;)) 大功告成，tf还是要多写，不然容易漏东西。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记2(变量与计算图搭建)]]></title>
    <url>%2F2018%2F03%2F08%2FTensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B02(%E5%8F%98%E9%87%8F%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE%E6%90%AD%E5%BB%BA)%2F</url>
    <content type="text"><![CDATA[常量、变量和占位符在定义计算图之前要了解一下tf中变量保存的形式。TensorFlow 中最基本的单位是常量（Constant）、变量（Variable）和占位符（Placeholder）。常量定义后值和维度不可变，变量定义后值可变而维度不可变。在神经网络中，变量一般可作为储存权重和其他信息的矩阵，而常量可作为储存超参数或其他结构信息的变量。在上面的计算图中，结点 1 和结点 2 都是定义的常量 tf.constant()。我们可以分别声明不同的常量（tf.constant()）和变量（tf.Variable()），其中 tf.float和tf.int分别声明了不同的浮点型和整数型数据。通常常量用来保存计算图中可训练的数据，常量则是不可训练数据。占位符的特点是没有初始值，它只会分配必要的内存。在会话中，占位符可以使用 feed_dict 馈送数据。占位符一般用来读取训练或者预测所用的数据，并送入图中，以及接受最后计算得到的结果。feed_dict 是一个字典，在字典中需要给出每一个用到的占位符的取值。12345678910111213import tensorflow as tfimport numpy as npa=tf.placeholder('float32', [3])b=tf.placeholder('float32', [3])c=tf.multiply(a, b)x1 = [np.random.random() for _ in range(3)]x2 = [np.random.random() for _ in range(3)]with tf.Session() as sess: print(sess.run(c, feed_dict=&#123;a:x1,b:x2&#125;)) 在初始化这些张量的时候一定要注意shape，整个计算图中只要有一个地方对不上，就会报错。 计算图TensorFlow 是一种采用数据流图（data flow graphs），用于数值计算的开源软件库。其中 Tensor 代表传递的数据为张量（多维数组），Flow 代表使用计算图进行运算。数据流图用「结点」（nodes）和「边」（edges）组成的有向图来描述数学运算。「结点」一般用来表示施加的数学操作，但也可以表示数据输入的起点和输出的终点，或者是读取/写入持久变量（persistent variable）的终点。边表示结点之间的输入/输出关系。这些数据边可以传送维度可动态调整的多维数据数组，即张量（tensor）。简单来说，计算图定义了从input到output过程中张量所经过的计算和变化。在tensorflow中用的是静态图。静态的意思就是计算图的声明和执行是分开的，我们接下来可以看到使用tf之前，整个计算的流程就已经被设计好了。这份计算图一般会包含计算执行顺序和内存空间分配的策略，这些策略的制定一般是这个过程中最消耗时间的部分；执行阶段构建的图叫实体计算图，这个过程包括为参数和中间结果实际分配内存空间，并按照当前需求进行计算等，数据就在这张实体计算图中计算和传递。而PyTorch用的则是动态图，声明和执行一起进行的。上代码：先声明需要拟合的数据，看一下他们的分布1234567891011121314151617import tensorflow as tfimport numpyimport matplotlib.pyplot as pltrng = numpy.random# Parameterslearning_rate = 0.01training_epochs = 2000display_step = 50# Training Datatrain_X = numpy.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])train_Y = numpy.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3])n_samples = train_X.shape[0]plt.plot(train_X, train_Y, '.')plt.show() 这么一看大概就能画出来了。 接下来定义各种变量。12345678# tf Graph InputX = tf.placeholder("float")Y = tf.placeholder("float")# Create Model# Set model weightsW = tf.Variable(rng.randn(), name="weight")b = tf.Variable(rng.randn(), name="bias") 然后定义计算图和优化对象，这是最重要的一部分，定义张量计算的方式和传递的路径。这个模型很简单，只有一步，Y=X×W+b。定义完计算图就能通过得到结果了，这时候可以定义损失函数了，因为是线性回归，用的就是均方差。优化器是普通的梯度下降。123456# Construct a linear modelactivation = tf.add(tf.multiply(X, W), b)# Minimize the squared errorscost = tf.reduce_sum(tf.pow(activation-Y, 2))/(2*n_samples) #L2 lossoptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) #Gradient descent 在正式训练开始之前需要初始化所有变量，其实就是分配内存的过程。这在上面静态图的部分说过了。喂数据用的是feed_dict上面也写过了。就可以开始训练了。123456789101112131415161718192021222324252627# Initializing the variablesinit = tf.global_variables_initializer()# Launch the graphwith tf.Session() as sess: sess.run(init) # Fit all training data for epoch in range(training_epochs): for (x, y) in zip(train_X, train_Y): sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;) #Display logs per epoch step if epoch % display_step == 0: print ("Epoch:", '%04d' % (epoch+1), "cost=", \ "&#123;:.9f&#125;".format(sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;)), \ "W=", sess.run(W), "b=", sess.run(b)) print("Optimization Finished!") print("cost=", sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), \ "W=", sess.run(W), "b=", sess.run(b)) #Graphic display plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() 最后的结果： 模型的保存和读取用tf.train.Saver()创建一个Saver来管理模型中的所有变量，这样的话方便下一次继续使用训练好的模型。在训练阶段最后加上一句把模型存下来。存下来的模型是一个ckpt文件，里面包含了上面定义的所有变量，所以如果重新读取使用的话要定义好计算图。12345678910111213141516171819# Launch the graphwith tf.Session() as sess: sess.run(init) # Fit all training data for epoch in range(training_epochs): for (x, y) in zip(train_X, train_Y): sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;) #Display logs per epoch step if epoch % display_step == 0: print ("Epoch:", '%04d' % (epoch+1), "cost=", \ "&#123;:.9f&#125;".format(sess.run(cost, feed_dict=&#123;X: train_X, Y:train_Y&#125;)), \ "W=", sess.run(W), "b=", sess.run(b)) # Save the variables to disk. save_path = saver.save(sess, "./tmp/model.ckpt") print("Model saved in file: ", save_path) print("Optimization Finished!") 然后可以直接读取训练好的模型进行预测，读取网上一些训练好的模型来做迁移学习也可以这么用。12345678910111213#restore modelwith tf.Session() as sess: # Restore variables from disk. saver.restore(sess, "./tmp/model.ckpt") print("Model restored.") print("cost=", sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), \ "W=", sess.run(W), "b=", sess.run(b)) #Graphic display plt.plot(train_X, train_Y, 'ro', label='Original data') plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line') plt.legend() plt.show() 引用https://zhuanlan.zhihu.com/p/29936078https://zhuanlan.zhihu.com/p/25216368https://github.com/aymericdamien/TensorFlow-Examples/http://www.tensorfly.cn/tfdoc/how_tos/variables.html]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记1(Tensor和基本计算）]]></title>
    <url>%2F2018%2F03%2F08%2FTensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B01(Tensor%E5%92%8C%E5%9F%BA%E6%9C%AC%E8%AE%A1%E7%AE%97%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Hello World创建一个session，它是tf对所有对象操作所需的基本单元。Session 是 Tensorflow 为了控制，和输出文件的执行的语句。 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分。1234import tensorflow as tfhello = tf.constant('Hello, TensorFlow!')sess = tf.Session()print sess.run(hello) TensorTensor实际上就是一个多维数组（multidimensional array），是TF的主要数据结构。它们在一个或多个由节点（nodes）和边（edges）组成的图（graphs）中流动。边代表的是tensors，节点代表的是对tensors的操作（operations，or Ops for short）。tensors在图中从一个节点流向另一个节点，每次经过一个节点都会接受一次操作。在计算机视觉的应用中，tensor一般有(dimmension, height, width)三个维度，比如一张三维256*256的图片，tensor的shape就是（3,256,256)。有时候还包含第四维batch size，用于批量学习。 创建tensor的方法可以分为2种，一种是用TF自带的一些函数直接创建，例如：12345678910111213import tensorflow as tf # create a zero filled tensor tf.zeros([row_dim, col_dim]) # create a one filled tensor tf.ones([row_dim, col_dim]) # create a constant filled tensor tf.fill([row_dim, col_dim], 42) # create a tensor out of an existing constant tf.constant([1, 2, 3]) # generate random numbers from a uniform distribution tf.random_uniform([row_dim, col_dim], minval=0, maxval=1) # generate random numbers from a normal distribution tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0) 另一种是将Python对象（Numpy arrays， Python lists，Python scalars）转成tensor，例如：123import numpy as npx_data = np.array([[1., 2., 3.], [3., 2., 6.]])tf.convert_to_tensor(x_data, dtype=tf.float32) Tensor对象有3个属性： 一个名字，它用于键值对的存储，用于后续的检索：Const: 0 一个形状描述， 描述数据的每一维度的元素个数：（6，3，7） 数据类型，比如 float32下面举一个简单的例子来说明tensor是如何在graph中流动的：节点a接收了一个1-D tensor，该tensor从节点a流出后，分别流向了节点b和c，节点b执行的是prod操作（5*3），节点c执行的是sum操作（5+3）。当tensor从节点b流出时变成了15，从节点c流出时变成了8。此时，2个tensor又同时流入节点d，接受的是add操作（15+8），最后从节点d流出的tensor就是23。用TF代码来创建上面的graph：12345import tensorflow as tf a = tf.constant([5, 3], name='input_a') b = tf.reduce_prod(a, name='prod_b') c = tf.reduce_sum(a, name='sum_c') d = tf.add(b, c, name='add_d') 在上面的代码中，我们用不同的构造函数（constructor）定义了四个操作（对应图上4个节点）。例如，tf.constant()创建的操作实际上就是一个“二传手”：接受一个tensor（或者一个list对象，自动将其转换成tensor对象），然后传给与它直接相连的下一个node。tf.reduce_prod()和tf.reduce_sum()操作可以把input tensor对象中的所有值相乘或相加，然后传递给下一个直接相连的node。 基本运算首先是int加减乘除。12345678import tensorflow as tfa = tf.constant(2)b = tf.constant(3)#一定要在一个session下进行tf的所有计算, 或者说所有动态操作。计算图搭建的过程倒是不需要with tf.Session() as sess: print('a=%i,b=%i)%(sess.run(a),sess.run(b))) print('Addtition:%i'%sess.run(a+b)) print('Addtition:%i'%sess.run(a+b)) 然后进行tensor的计算，tensor可以说是tf计算的最基本操作对象了，毕竟名字就叫tensorflow，对tensor的运算有多重要不言而喻。1234567891011import tensorflow as tfrow_dim = 3col_dim=3# generate random numbers from a uniform distributiona=tf.random_uniform([row_dim, col_dim], minval=0, maxval=1)# generate random numbers from a normal distributionb=tf.random_normal([row_dim, col_dim], mean=0.0, stddev=1.0)with tf.Session() as sess: print((sess.run(a), sess.run(b))) print(sess.run(a*b)) print(sess.run(a+b)) 数学运算可能会用到的一些函数：12345678910111213tf.addtf.subtf.multiply #这个和*一样的。如果shape一样对应元素相乘。不一样的时候broadcast。tf.divtf.modtf.abs tf.negtf.signtf.invtf.squaretf.roundtf.sqrt tf.pow tf.exp tf.log tf.maximumtf.minimumtf.cos tf.sin 对二维张量的一些计算：12345tf.diag生成对角阵tf.transpose转置tf.matmul矩阵乘法 #（2,3）×（3,2）-&gt;(2,2)tf.matrix_determinant计算行列式的值tf.matrix_inverse计算矩阵的逆 引用https://zhuanlan.zhihu.com/p/30487008https://www.jianshu.com/p/6fec37e6ccc1]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow学习笔记0（简介）]]></title>
    <url>%2F2018%2F03%2F07%2FTensorflow%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B00%EF%BC%88%E7%AE%80%E4%BB%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Preface之前其实也用了快一年Tensorflow, 但是实际上很多具体的实现并没有很清楚, 所以决定从头学一遍，边学变记吧。会参考一些别的教程，结合自己的理解，把学习的内容整理下来和大家分享。从基本的变量，计算图的搭建开始，后面实现一些常用的网络结构，Alexnet， VGG， Resnet之类。争取一个月之内全部更完。这一系列笔记主要针对的是tf模型和api的复习，所以关于神经网络方面的内容就不深入了写了。 笔记目录 tf基本元素 变量与计算图搭建 mnist和cnn tensorboard可视化 Alexnet tf.slim VGG Resnet]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2018机器学习实习面试知识点整理]]></title>
    <url>%2F2018%2F03%2F06%2F2018%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E4%B9%A0%E9%9D%A2%E8%AF%95%E7%9F%A5%E8%AF%86%E7%82%B9%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[算是第一次正式的跟职场接触, 刚开始的时候也是一头雾水的状态。面得多了也慢慢有经验, 除了第一个头条挂了, 嗯挂的很惨。也是很想去的一家公司, 但是第一次哎, 多半会这样, 有种似曾相识的感觉。后面的除了阿里还没有回复, 一共拿了5个offer, 还算满意了吧。有些遗憾的是因为时间不合适, 没有去尝试一些大厂, 下次一定要补上。 这里总结的问题一部分是我自己遇到的, 还有一大部分是网上看了别人的面经摘抄或者总结出来的。还有一些有意思的问题, 因为懒也没有来得及写上, 以后会慢慢补充。很多地方可能考虑不周, 或者有别的角度, 欢迎大家补充或者指出。面试准备:针对简历问题项目用到的特征 Auto Color Correlogram (acc)颜色自动相关图（color auto-correlogram），它仅仅考察具有相同颜色的像素间的空间关系 Color and Edge Directivity Descriptor (cedd)CEDD结合了图像的颜色和纹理信息，生成一个144位的直方图。1.1.RGB模型转换为HSV模型H （Hue）代表色调，指通过物体传播或从物体射出的颜色，一般在使用中是由颜色名称来标识的。S （Saturation）代表饱和度，表示色调中灰色成分的比例，指颜色的纯度或强度。V （Value）代表亮度. Color Layout (cl)Color Layout Descriptor是mpeg-7多媒体内容标准描述中一种高效的局部颜色特征描述，在基于内容的图像检索(Content Based Image Retrieval (CBIR) ) 中表现出很好性能，拥有计算成本低，匹配计算速度快，识别准确率高等优点。可以用来做以图搜图 Edge Histogram (eh) Fuzzy Color and Texture Histogram (fcth)模糊颜色和纹理直方图 Gabor (gabor)Gabor 特征是一种可以用来描述图像纹理信息的特征，Gabor 滤波器的频率和方向与人类的视觉系统类似，特别适合于纹理表示与判别。从图像处理的角度来看，Gabor特征有如下好处：（1）Gabor核函数由于去掉了直流分量，因此对局部光照的变化不敏感，常常被用在要求对光照有适应性的场合；（2）Gabor滤波结果可以反映图像不同尺度、不同方向上的灰度分布信息。一般说来，大尺度滤波可以反映全局性较强的信息，同时可以掩盖图像中噪声的影响；小尺度可以反映比较精细的局部结构，但容易受到噪声影响。 Joint descriptor joining CEDD and FCTH in one histogram (jcd) Scalable Color (sc)基于MPEG-7推荐的可伸缩颜色描述符 Tamura (tamura)Tamura 纹理特征基于人类对纹理的视觉感知心理学研究，提出6种属性，即：粗糙度、对比度、方向度、线像度、规整度和粗略度。 Local Binary Patterns (lbp) LBP特征(从灰度图提取)下图反应了某个像素的具体的LBP特征值的计算过程，需要注意的是，LBP值是按照顺时针方向组成的二进制数。用公式来定义的话，如下所示：其中代表3x3邻域的中心元素，它的像素值为ic，ip代表邻域内其他像素的值。s(x)是符号函数，定义如下： HOG特征(从灰度图提取)方向梯度直方图（Histogram of Oriented Gradient, HOG）计算图像横坐标和纵坐标方向的梯度，并据此计算每个像素位置的梯度方向值；求导操作不仅能够捕获轮廓，人影和一些纹理信息，还能进一步弱化光照的影响。图像中像素点(x,y)的梯度为： 最常用的方法是：首先用[-1,0,1]梯度算子对原图像做卷积运算，得到x方向（水平方向，以向右为正方向）的梯度分量gradscalx，然后用[1,0,-1]T梯度算子对原图像做卷积运算，得到y方向（竖直方向，以向上为正方向）的梯度分量gradscaly。然后再用以上公式计算该像素点的梯度大小和方向。（3）为每个细胞单元构建梯度方向直方图（4）把细胞单元组合成大的块（block），块内归一化梯度直方图 ORB特征(提取特征点)ORB：An Efficient Alternative to SIFT or SURF 调参目的：偏差和方差的协调！！！bias，variance原则：模型从简单到复杂，避免过拟合传统方法：1）树类分类器n_estimators:子模型数量max_features:每个子树能用到的最大特征。一般去总特征数开方或者log或者全部max_depth:最大深度min_sample_split:min_sample_leaf:神经网络：learning rate: 1 0.1 0.01 0.001, 一般从1开始尝试。很少见learning rate大于10的。学习率一般要随着训练进行衰减。衰减系数一般是0.5。 衰减时机，可以是验证集准确率不再上升时，或固定训练多少个周期以后。不过更建议使用自适应梯度的办法，例如adam,adadelta,rmsprop等，这些一般使用相关论文提供的默认值即可，可以避免再费劲调节学习率。对RNN来说，有个经验，如果RNN要处理的序列比较长，或者RNN层数比较多，那么learning rate一般小一些比较好，否则有可能出现结果不收敛，甚至Nan等问题。网络层数： 先从1层开始。每层结点数： 16 32 128，超过1000的情况比较少见。超过1W的从来没有见过。batch size: 128上下开始。batch size值增加，的确能提高训练速度。但是有可能收敛结果变差。如果显存大小允许，可以考虑从一个比较大的值开始尝试。因为batch size太大，一般不会对结果有太大的影响，而batch size太小的话，结果有可能很差。clip c(梯度裁剪): 限制最大梯度,其实是value = sqrt(w1^2+w2^2….),如果value超过了阈值，就算一个衰减系系数,让value的值等于阈值: 5,10,15dropout： 0.5L2正则：1.0，超过10的很少见。正负样本比例： 这个是非常忽视，但是在很多分类问题上，又非常重要的参数。很多人往往习惯使用训练数据中默认的正负类别比例，当训练数据非常不平衡的时候，模型很有可能会偏向数目较大的类别，从而影响最终训练结果。除了尝试训练数据默认的正负类别比例之外，建议对数目较小的样本做过采样，例如进行复制。提高他们的比例，看看效果如何，这个对多分类问题同样适用。在使用mini-batch方法进行训练的时候，尽量让一个batch内，各类别的比例平衡，这个在图像识别等多分类任务上非常重要。 什么是boosting tree（加法模型+前向分布）Boosting方法： Boosting这其实思想相当的简单，大概是，对一份数据，建立M个模型（比如分类），一般这种模型比较简单，称为弱分类器(weak learner)每次分类都将上一次分错的数据权重提高一点再进行分类，这样最终得到的分类器在测试数据与训练数据上都可以得到比较好的成绩。前向分布算法 实际上是一个贪心的算法，也就是在每一步求解弱分类器Φ(m)和其参数w(m)的时候不去修改之前已经求好的分类器和参数。用当前模型的残差，即r=y-fm(x), 来计算下一颗树的参数。 GBDT梯度提升与普通boosting的区别是，利用的是损失函数的负梯度在当前函数的值来拟合回归树。算法： XGB传统GBDT在优化时只用到一阶导数信息，xgboost则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。顺便提一下，xgboost工具支持自定义代价函数，只要函数可一阶和二阶求导。xgboost在代价函数里加入了正则项，用于控制模型的复杂度。正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和。从Bias-variance tradeoff角度来讲，正则项降低了模型的variance，使学习出来的模型更加简单，防止过拟合，这也是xgboost优于传统GBDT的一个特性。xgboost工具支持并行。xgboost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。 L1和L2正则为什么可以减弱overfitting欠拟合(underfitting)，或者叫作叫做高偏差(bias).过拟合(overfitting)，也叫高方差(variance).越简单的模型泛化能力越强。对高阶项进行一定的惩罚，避免模型在数据量不够的时候过于复杂。Ps：防止过拟合的其他方法，early stopping、数据集扩增（Data augmentation），dropoutL1和L2正则有什么区别L1范数和L0范数可以实现稀疏（使没用的特征为0），L1因具有比L0更好的优化求解特性而被广泛应用。L1使损失函数可导。L2范数不但可以防止过拟合，还可以让我们的优化求解变得稳定和快速。L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。为什么L1正则可以实现参数稀疏，而L2正则不可以？答了：L1正则因为是绝对值形式，很多系数被压缩为0,。而L2正则是很多系数被压迫到接近于0，而不是0为什么L1很多系数可以被压缩为0，L2是被压缩至接近于0？答了：图像上，L1正则是正方形，L2正则是圆形。L1正则的往往取到正方形顶点，即有很多参数为0.L2正则往往去不到圆形和参数线的交点，即很多分量被压缩到接近于0 怎么理解dropout在每次训练的时候使用dropout，每个神经元有一定的概率被移除，这样可以使得一个神经元的训练不依赖于另外一个神经元，同样也就使得特征之间的协同作用被减弱。Hinton认为，过拟合可以通过阻止某些特征的协同作用来缓解。增加鲁棒性。也可以理解为相当于在训练不同的网络，最后投票来决定结果。 KNN和Lr有什么本质区别LR属于线性模型。因为 logistic 回归的决策边界（decision boundary）是线性的。KNN属于非线性模型 学习率如何影响训练？如果学习率很低，训练会变得更加可靠，但是优化会耗费较长的时间，因为朝向损失函数最小值的每个步长很小。如果学习率很高，训练可能根本不会收敛，甚至会发散。权重的改变量可能非常大，使得优化越过最小值，使得损失函数变得更糟。 如何解决样本不均衡的问题?从数据集外补充, 我的项目里直接从网上爬图片, 加上SMOTE等过采样的算法(有好多种, 各有优劣, SMOTE应该是最好的), 以及对图片进行一些几何上的处理。 线性分类器和非线性分类器线性分类器：模型是参数的线性函数，分类平面是（超）平面；非线性分类器：模型分界面可以是曲面或者超平面的组合。典型的线性分类器有感知机，LDA，逻辑斯特回归，SVM（线性核；典型的非线性分类器有朴素贝叶斯（有文章说这个本质是线性的，http://dataunion.org/12344.html），kNN，决策树，SVM（非线性核） 为什么random forest具有特征选择的功能树类分类器其实都可以, 因为可以判断每个特征对应的信息增益, 信息增益越大的特征分类效果理论上来说就越好。在非结构话数据的项目中常常会用这种方法来选取有用的特征, 从而降低特征维度。在特特征维度高的时候还是特别好用的。 random forest有哪些重要的参数？A. max_features：随机森林允许单个决策树使用特征的最大数量。 Python为最大特征数提供了多个可选项。 下面是其中的几个：Auto/None ：简单地选取所有特征，每颗树都可以利用他们。这种情况下，每颗树都没有任何的限制。sqrt ：此选项是每颗子树可以利用总特征数的平方根个。 例如，如果变量（特征）的总数是100，所以每颗子树只能取其中的10个。“log2”是另一种相似类型的选项。0.2：此选项允许每个随机森林的子树可以利用变量（特征）数的20％。如果想考察的特征x％的作用， 我们可以使用“0.X”的格式。max_features如何影响性能和速度？增加max_features一般能提高模型的性能，因为在每个节点上，我们有更多的选择可以考虑。 然而，这未必完全是对的，因为它降低了单个树的多样性，而这正是随机森林独特的优点。 但是，可以肯定，你通过增加max_features会降低算法的速度。 因此，你需要适当的平衡和选择最佳max_features。B. n_estimators：在利用最大投票数或平均值来预测之前，你想要建立子树的数量。 较多的子树可以让模型有更好的性能，但同时让你的代码变慢。 你应该选择尽可能高的值，只要你的处理器能够承受的住，因为这使你的预测更好更稳定。C. min_sample_leaf：如果您以前编写过一个决策树，你能体会到最小样本叶片大小的重要性。 叶是决策树的末端节点。 较小的叶子使模型更容易捕捉训练数据中的噪声。 一般来说，我更偏向于将最小叶子节点数目设置为大于50。在你自己的情况中，你应该尽量尝试多种叶子大小种类，以找到最优的那个。 为什么梯度反方向是函数值局部下降最快的方向？那么此时如果要取得最大值，也就是当为0度的时候，也就是向量(这个方向是一直在变，在寻找一个函数变化最快的方向）与向量（这个方向当点固定下来的时候，它就是固定的）平行的时候，方向导数最大.方向导数最大，也就是单位步伐，函数值朝这个反向变化最快. DNN为什么功能强大，说说你的理解深度学习的深度一方面增加了大量的参数，增加的参数意味着这个网络的表达能力更强大了。可以学习和区分的特征更多了。而一旦学习到的特征变多的话，我们在分类和识别的能力也就变好了。从简单特征到抽象特征。随着网络深度增加，提取的特征不断复杂化。更能理解复杂概念。 SVM的损失函数是什么？怎么理解2分类SVM等于Hinge损失 + L2正则化。SVM最大化分类间距的目标函数等价于最小化Hinge损失 + L2正则化。推导并不复杂, 详见《统计学习方法》。 介绍下Maxoutmaxout激活函数，它具有如下性质：1、maxout激活函数并不是一个固定的函数，不像Sigmod、Relu、Tanh等函数，是一个固定的函数方程2、它是一个可学习的激活函数，因为我们W参数是学习变化的。3、它是一个分段线性函数：maxout的拟合能力是非常强的，它可以拟合任意的的凸函数。最直观的解释就是任意的凸函数都可以由分段线性函数以任意精度拟合（学过高等数学应该能明白），而maxout又是取k个隐隐含层节点的最大值，这些”隐隐含层”节点也是线性的，所以在不同的取值范围下，最大值也可以看做是分段线性的（分段的个数与k值有关） 根据混淆矩阵可以得到评价分类模型的指标有以下几种。分类准确度，就是正负样本分别被正确分类的概率，计算公式为：召回率，就是正样本被识别出的概率，计算公式为：虚警率，就是负样本被错误分为正样本的概率，计算公式为：精确度，就是分类结果为正样本的情况真实性程度，计算公式为： 优化器类型 Batch gradient descent梯度更新规则:BGD 采用整个训练集的数据来计算 cost function 对参数的梯度：Batch gradient descent 对于凸函数可以收敛到全局极小值，对于非凸函数可以收敛到局部极小值。 Stochastic gradient descent梯度更新规则:和 BGD 的一次用所有数据计算梯度相比，SGD 每次更新时对每个样本进行梯度更新，对于很大的数据集来说，可能会有相似的样本，这样 BGD 在计算梯度时会出现冗余，而 SGD 一次只进行一次更新，就没有冗余，而且比较快，并且可以新增样本。缺点:但是 SGD 因为更新比较频繁，会造成 cost function 有严重的震荡。BGD 可以收敛到局部极小值，当然 SGD 的震荡可能会跳到更好的局部极小值处。 3.Adam自适应优化器，能够自发地改变学习率。效果最好。存储了过去梯度的平方 vt 的指数衰减平均值 ，也像 momentum 一样保持了过去梯度 mt 的指数衰减平均值：梯度更新规则:超参数设定值:建议 β1 ＝ 0.9，β2 ＝ 0.999，ϵ ＝ 10e−8实践表明，Adam 比其他适应性学习方法效果要好 什么是梯度消失？怎么解决梯度消失问题发生时，接近于输出层的hidden layer 3等的权值更新相对正常，但前面的hidden layer 1的权值更新会变得很慢，导致前面的层权值几乎不变，仍接近于初始化的权值，这就导致hidden layer 1相当于只是一个映射层，对所有的输入做了一个同一映射，这是此深层网络的学习就等价于只有后几层的浅层网络的学习了。其实梯度爆炸和梯度消失问题都是因为网络太深，链式求导，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。对于更普遍的梯度消失问题，可以考虑用ReLU激活函数取代sigmoid激活函数。 batch normalisation作用那BN到底是什么原理呢？说到底还是为了防止“梯度弥散”。关于梯度弥散，大家都知道一个简单的栗子：在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法。 LSTM模型介绍和BPTT推导 LR 和 linear SVM区别相同点：1，LR和SVM都是分类算法。2，如果不考虑核函数，LR和SVM都是线性分类算法，即分类决策面都是线性的。3，LR和SVM都是监督学习算法。不同点：1，本质上是其loss function不同。2，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用）。线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。3，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。这个问题理解起来非常简单。分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。4，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。5，SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！在Andrew NG的课里讲到过： 如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM 如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel 如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况 SVM面试问题为什么要把原问题转换为对偶问题？因为原问题是凸二次规划问题，转换为对偶问题更加高效。为什么求解对偶问题更加高效？因为只用求解alpha系数，而alpha系数只有支持向量才非0，其他全部为0.alpha系数有多少个？样本点的个数 线性回归基本假设?线性回归需要满足四个前提假设.LINE!!! Linearity 线性. 应变量和每个自变量都是线性关系。 Indpendence 独立性. 对于所有的观测值，它们的误差项相互之间是独立的。 Normality 正态性. 误差项服从正态分布。 Equal-variance 等方差. 所有的误差项具有同样方差。 用cos做激活函数行不行?答案是可以的, 面试的时候想太多答得不好。当时觉得cos容易使得模型出现梯度消失的情况, 但是其实要看具体的问题, cos在某些问题上表现得很好。其实激活函数的目的是为了使模型具有非线性, 否则再深的神经网络到最后也只是一个线性分类器。一个好的激活函数应该有如下几个标准(摘自知乎):作者：Hengkai Guo链接：https://www.zhihu.com/question/67366051/answer/262087707来源：知乎著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 非线性：即导数不是常数。这个条件前面很多答主都提到了，是多层神经网络的基础，保证多层网络不退化成单层线性网络。这也是激活函数的意义所在。 几乎处处可微：可微性保证了在优化中梯度的可计算性。传统的激活函数如sigmoid等满足处处可微。对于分段线性函数比如ReLU，只满足几乎处处可微（即仅在有限个点处不可微）。对于SGD算法来说，由于几乎不可能收敛到梯度接近零的位置，有限的不可微点对于优化结果不会有很大影响[1]。 计算简单：正如题主所说，非线性函数有很多。极端的说，一个多层神经网络也可以作为一个非线性函数，类似于Network In Network[2]中把它当做卷积操作的做法。但激活函数在神经网络前向的计算次数与神经元的个数成正比，因此简单的非线性函数自然更适合用作激活函数。这也是ReLU之流比其它使用Exp等操作的激活函数更受欢迎的其中一个原因。 非饱和性（saturation）：饱和指的是在某些区间梯度接近于零（即梯度消失），使得参数无法继续更新的问题。最经典的例子是Sigmoid，它的导数在x为比较大的正值和比较小的负值时都会接近于0。更极端的例子是阶跃函数，由于它在几乎所有位置的梯度都为0，因此处处饱和，无法作为激活函数。ReLU在x&gt;0时导数恒为1，因此对于再大的正值也不会饱和。但同时对于x&lt;0，其梯度恒为0，这时候它也会出现饱和的现象（在这种情况下通常称为dying ReLU）。Leaky ReLU[3]和PReLU[4]的提出正是为了解决这一问题。 单调性（monotonic）：即导数符号不变。这个性质大部分激活函数都有，除了诸如sin、cos等。个人理解，单调性使得在激活函数处的梯度方向不会经常改变，从而让训练更容易收敛。 输出范围有限：有限的输出范围使得网络对于一些比较大的输入也会比较稳定，这也是为什么早期的激活函数都以此类函数为主，如Sigmoid、TanH。但这导致了前面提到的梯度消失问题，而且强行让每一层的输出限制到固定范围会限制其表达能力。因此现在这类函数仅用于某些需要特定输出范围的场合，比如概率输出（此时loss函数中的log操作能够抵消其梯度消失的影响[1]）、LSTM里的gate函数。7. 接近恒等变换（identity）：即约等于x。这样的好处是使得输出的幅值不会随着深度的增加而发生显著的增加，从而使网络更为稳定，同时梯度也能够更容易地回传。这个与非线性是有点矛盾的，因此激活函数基本只是部分满足这个条件，比如TanH只在原点附近有线性区（在原点为0且在原点的导数为1），而ReLU只在x&gt;0时为线性。这个性质也让初始化参数范围的推导更为简单[5][4]。额外提一句，这种恒等变换的性质也被其他一些网络结构设计所借鉴，比如CNN中的ResNet[6]和RNN中的LSTM。 参数少：大部分激活函数都是没有参数的。像PReLU带单个参数会略微增加网络的大小。还有一个例外是Maxout[7]，尽管本身没有参数，但在同样输出通道数下k路Maxout需要的输入通道数是其它函数的k倍，这意味着神经元数目也需要变为k倍；但如果不考虑维持输出通道数的情况下，该激活函数又能将参数个数减少为原来的k倍。 归一化（normalization）：这个是最近才出来的概念，对应的激活函数是SELU[8]，主要思想是使样本分布自动归一化到零均值、单位方差的分布，从而稳定训练。在这之前，这种归一化的思想也被用于网络结构的设计，比如Batch Normalization[9]。 To be solved…项目中over-fitting了，你怎么办详细说一个你知道的优化算法(Adam等)项目(比赛）怎么做的模型的ensemblestacking是什么？需要注意哪些问题了解哪些online learning的算法如何解决样本不均衡的问题fasterRCNN中的ROIPooling是如何实现的如何进行特征的选择如何进行模型的选择常用的有哪些损失函数XX用户画像挖掘怎么做的feature engineering?假设一个5*5的filter与图像卷积，如何降低计算量？做过模型压缩吗？介绍下什么是residual learning？说说你的理解residual learning所说的residual和GBDT中的residual有什么区别？FFM和FTRL有过了解吗？你对现在Deep Learning的发展和遇到的问题有什么看法？]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写在开始]]></title>
    <url>%2F2018%2F03%2F05%2F%E5%86%99%E5%9C%A8%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[其实写一个个人网站是一个已经酝酿许久的想法, 但是一直太懒, 也没有时间;), 所以就耽搁下了。这次趁着在法国生活即将结束之际, 偷懒地用github把这个小计划也实现了。以后会在这里写些技术文章, 机器学习, 算法, AI估计都会有, 希望能和大家共同进步吧, 和一些无病呻吟的文章, 毕竟生活还需要记录和一些仪式感。马上就要回国了, 总算可以专心写代码学算法了, 之前学的时候都太仓促, 以后要多总结下。最后, 要把写博客的习惯坚持下去。以上]]></content>
      <categories>
        <category>杂记</category>
      </categories>
      <tags>
        <tag>无病呻吟</tag>
      </tags>
  </entry>
</search>
